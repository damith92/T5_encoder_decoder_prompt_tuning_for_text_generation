{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254c87e3-a0e5-419e-9e9e-4ef151510973",
   "metadata": {},
   "source": [
    "# Prompt tuning model implementation for steering the T5 with encoder-decoder prompts to produce positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d61cff-e2e2-492c-abcc-6c7781ed10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import string\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "import glob\n",
    "import sys\n",
    "import io\n",
    "\n",
    "import zipfile\n",
    "import tarfile\n",
    "\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8733c7-4ff1-4bf6-a71c-bd95cb8e4833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93bd3d5-ac2d-4a8c-b998-a85780ea7336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set one cuda visible device if multiple GPUs are avialable\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ac079f-1927-4f85-b129-e8d193ffe85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5TokenizerFast,\n",
    "    get_scheduler\n",
    ")\n",
    "import torch\n",
    "\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model_classes.model_t5_encoder_decoder_prompt import T5PromptTuningLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5129f5-bd14-4d1d-b1cf-774ad1b2a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the seed to be able to get the same randomness across runs and hence reproducible outcomes\n",
    "def get_device_and_set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    return device\n",
    "    \n",
    "SEED = 123\n",
    "device = get_device_and_set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eaca36-7924-4711-aff5-5db145ea19f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    num_train_epochs = 20\n",
    "    learning_rate = 0.15\n",
    "    warmup_steps = 500\n",
    "    max_train_steps = num_train_epochs\n",
    "    weight_decay=0.01\n",
    "    batch_size = 10\n",
    "    # Prompt-tuning\n",
    "    # number of prompt tokens\n",
    "    n_prompt_tokens = 20\n",
    "    # If True, soft prompt will be initialized from vocab \n",
    "    # Otherwise, you can set `random_range` to initialize by randomization.\n",
    "    init_from_vocab = True\n",
    "    # random_range = 0.5\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddc5a72-8f6c-462f-bc4b-d04d37ce22dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee46c9-4bae-4ba6-b518-986a5525e91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(\"google/t5-small-lm-adapt\")\n",
    "# Load the model\n",
    "model_pos = T5PromptTuningLM.from_pretrained(\n",
    "    \"google/t5-small-lm-adapt\",\n",
    "    encoder_soft_prompt_path=\"../trained_models/t5_encoder_decoder/positive/encoder_soft_prompt_T5_pos.model\",\n",
    "    decoder_soft_prompt_path=\"../trained_models/t5_encoder_decoder/positive/decoder_soft_prompt_T5_pos.model\",\n",
    "    device=device\n",
    ").to(device)\n",
    "model_pos.eval()\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad07b0-b3fc-4ec1-a992-21072b18c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the model\n",
    "model_neg = T5PromptTuningLM.from_pretrained(\n",
    "    \"google/t5-small-lm-adapt\",\n",
    "    encoder_soft_prompt_path=\"../trained_models/t5_encoder_decoder/negative/encoder_soft_prompt_T5_neg.model\",\n",
    "    decoder_soft_prompt_path=\"../trained_models/t5_encoder_decoder/negative/decoder_soft_prompt_T5_neg.model\",\n",
    "    device=device\n",
    ").to(device)\n",
    "model_neg.eval()\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e3beb-e141-4f36-9595-3635737346ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_classes.de_generation_2 import DExpertsGenerationMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3cd44-7865-49e1-93ef-a5056979c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_mod = DExpertsGenerationMod(expert_model=model_pos, antiexpert_model=model_neg, device=device, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c72927-c4e3-4775-bf31-5ff879d73e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"the movie was\"\n",
    "call = tokenizer(test, return_tensors=\"pt\").input_ids\n",
    "\n",
    "input_ids = torch.tensor([call.tolist()[0][:-1]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52276a8a-d12c-4bc7-b923-b60d4b324e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([call.tolist()[0][:-1]]).to(device)\n",
    "decoder_input_ids = torch.zeros([1,1]).long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c975b-724c-4a52-b7ca-3a956b1ce0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12583092-1f6c-4cf6-bbef-c9209ec7d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_1 = de_mod.generate(input_ids = input_ids,\n",
    "                 decoder_input_ids = decoder_input_ids,\n",
    "                 max_len = 200,\n",
    "                 sample = True,\n",
    "                 filter_p = 1,\n",
    "                 k = 0,\n",
    "                 p = 0.9,\n",
    "                 temperature= 1.1,\n",
    "                 alpha = 1.2,   \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23064f0e-5507-421b-80c0-e2e9a604adef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(op_1[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278f451-ecdc-48f2-a2a1-bb27c0353f75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0c49d-7afd-4c0a-9d7d-a7ab3dd707c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_test = pd.read_csv(\"../data/2_data_remove_duplicates_5_pos_sampled_test.csv\", encoding='utf-8')\n",
    "test_texts = df_pos_test[\"reviewText\"].tolist() \n",
    "#test_texts = shuffle(np.array(test_texts), random_state=SEED)\n",
    "\n",
    "del df_pos_test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc77f3-ac0e-4959-bf90-f82abb089982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85071f7b-3f90-422f-9acb-2f52b20c2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, inp_perecentage=0.4):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inp_perecentage = inp_perecentage\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        encodings = self.tokenizer.encode(self.texts[idx], truncation=True, padding=True, return_tensors='pt')\n",
    "        item = {}\n",
    "        full_ids = encodings.tolist()[0][:-1]\n",
    "        \n",
    "        item[\"full_text\"]=self.texts[idx]\n",
    "        full_len = len(full_ids)\n",
    "    \n",
    "        input_len = math.floor(full_len*self.inp_perecentage)\n",
    "        \n",
    "        if input_len < 3:\n",
    "            input_len=3\n",
    "        elif input_len > 20:\n",
    "            input_len=20\n",
    "        \n",
    "        if full_len < 25:\n",
    "            full_len=25\n",
    "            \n",
    "        item[\"min_length\"] = (input_len+full_len)+args.n_prompt_tokens+80\n",
    "        item[\"max_length\"] = (input_len+full_len)+args.n_prompt_tokens+80\n",
    "        item[\"input_ids\"] = torch.tensor([full_ids[:input_len]]).to(device)\n",
    "        item[\"full_len\"] = full_len\n",
    "        item[\"input_len\"] = input_len\n",
    "        item[\"full_ids\"]= full_ids\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "\n",
    "test_dataset = AmazonDatasetTest(test_texts, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4c5b85-2d51-4e62-b9c2-c2b818ae743f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cdb2ce-1517-4c1b-bdca-a3601d079f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from torch import nn\n",
    "\n",
    "def bleu_score(li_abs_hyp, li_abs_ref):\n",
    "    \"\"\"\n",
    "    Computes the BLEU score\n",
    "    :param li_abs_hyp: list of hypothesis abstracts (token strings)\n",
    "    :param li_abs_ref: list of reference abstracts (token strings)\n",
    "    \"\"\"\n",
    "    bleu = corpus_bleu(li_abs_hyp, [li_abs_ref])\n",
    "\n",
    "    return bleu.score\n",
    "\n",
    "\n",
    "def rouge_score(li_abs_hyp, li_abs_ref):\n",
    "    \"\"\"\n",
    "    Computes the ROUGE score\n",
    "    :param li_abs_hyp: list of hypothesis abstracts (token strings)\n",
    "    :param li_abs_ref: list of reference abstracts (token strings)\n",
    "    \"\"\"\n",
    "    rouge_scores = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    for hyp, ref in zip(li_abs_hyp, li_abs_ref):\n",
    "        local_rouge_scores = scorer.score(ref, hyp)\n",
    "        for rouge_type in rouge_scores.keys():\n",
    "            rouge_scores[rouge_type] += local_rouge_scores[rouge_type].fmeasure\n",
    "\n",
    "    # Compute the averages \n",
    "    for rouge_type in rouge_scores.keys():\n",
    "        rouge_scores[rouge_type] = rouge_scores[rouge_type] / max(len(li_abs_hyp), 1e-7)\n",
    "    \n",
    "    return rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2eb054-00c7-4d8d-9158-6772c5d588a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c609443-a60a-4b6e-955a-47c4ad03cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_T5_gpu_2(net, data_iter, loss_model, device=None):\n",
    "    \"\"\"Compute the f1 score for a model on a dataset using a GPU.\n",
    "\n",
    "    Defined in :numref:`sec_lenet`\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # No. of correct predictions, no. of predictions\n",
    "\n",
    "    \n",
    "    y_tot =[]\n",
    "    y_hat_tot = []\n",
    "    y_hat_tot_plus = []\n",
    "    input_texts_ls = []\n",
    "    full_texts=[]\n",
    "    perplexities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for inputs in tqdm(data_iter, total=len(data_iter)):\n",
    "            \n",
    "            beam_outputs = net.generate(\n",
    "                input_ids = inputs[\"input_ids\"], \n",
    "                decoder_input_ids = torch.zeros([1,1]).long().to(device),\n",
    "                max_len = inputs[\"max_length\"],\n",
    "                sample = True,\n",
    "                filter_p = 1,\n",
    "                k = 0,\n",
    "                p = 0.9,\n",
    "                temperature= 1.1,\n",
    "                alpha = 1.2,\n",
    ")\n",
    "            \n",
    "            \n",
    "            y_hat = \" \".join(data_iter.tokenizer.decode(beam_outputs[0], skip_special_tokens=True).split()[:(inputs[\"full_len\"])]).lower()\n",
    "            y_hat_plus = \" \".join(data_iter.tokenizer.decode(beam_outputs[0], skip_special_tokens=True).split()[:(inputs[\"full_len\"]+inputs[\"input_len\"])]).lower()\n",
    "            input_text = data_iter.tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True ).lower()\n",
    "            \n",
    "            y_hat = y_hat.replace(input_text, \"\").strip()\n",
    "            y_hat_plus = \" \".join(y_hat_plus.replace(input_text, \"\").strip().split()[:(inputs[\"full_len\"])])\n",
    "            \n",
    "            #print(y_hat_plus,\"\\n\")\n",
    "            \n",
    "            y_tot += [inputs[\"full_text\"]]\n",
    "            #full_texts += [inputs[\"full_text\"]]\n",
    "            input_texts_ls += [input_text]\n",
    "            y_hat_tot += [y_hat]\n",
    "            y_hat_tot_plus += [y_hat_plus]\n",
    "            \n",
    "            \n",
    "            inps_2 = data_iter.tokenizer.encode(y_hat_plus)[:-1]\n",
    "            \n",
    "            labels_2 = torch.tensor([inps_2[1:][:511]+[1]]).to(device)\n",
    "            inputs_2 = torch.tensor([inps_2[:-1][:511]+[1]]).to(device)\n",
    "            mask_2 =  torch.tensor([[1]*inputs_2.shape[1]]).to(device)\n",
    "            \n",
    "            loss_model.eval()\n",
    "            #loss_model.to(device)\n",
    "            loss_2 = loss_model(input_ids=inputs_2, attention_mask=mask_2, decoder_input_ids=inputs_2, decoder_attention_mask=mask_2 , labels=labels_2.to(device)).get(\"loss\").detach() \n",
    "            \n",
    "            #loss_m = (loss_2 - loss_1) / (inputs_2.shape[1] - inputs_1.shape[1])\n",
    "            ppl = math.exp(loss_2.item())\n",
    "            if ppl < 1e4:   # for sanity\n",
    "                perplexities.append(ppl)\n",
    "                #print(\"added ppl = \", ppl)\n",
    "            else:\n",
    "                print(\"missed ppl = \", ppl)\n",
    "                \n",
    "            \n",
    "            \n",
    "    bleu_value = bleu_score(y_hat_tot_plus, y_tot)\n",
    "    try:\n",
    "        rouge_value = rouge_score(y_hat_tot_plus, y_tot)\n",
    "    except:\n",
    "        rouge_value = {\"rouge1\": 0.00}\n",
    "            \n",
    "\n",
    "    return bleu_value, rouge_value, y_hat_tot, y_hat_tot_plus, y_tot, input_texts_ls, np.nanmean(perplexities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5bced0-d875-4918-be55-23b4c785996f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761b4d6-6feb-4940-a23c-da73eda89295",
   "metadata": {},
   "outputs": [],
   "source": [
    "bl2, rg2, predictions2, preds_plus2, full_texts2, input_texts2, ppl2 = evaluate_T5_gpu_2(de_mod, test_dataset, model_pos )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686828dd-9e1d-40ec-b21d-1a70edc5eab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74854d37-ec57-4fb0-b00a-a0e38d7c843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinctness(generations):\n",
    "    \n",
    "    unigrams, bigrams, trigrams = set(), set(), set()\n",
    "    total_words = 0\n",
    "    for gen in generations:\n",
    "        o = gen.split(' ')\n",
    "        total_words += len(o)\n",
    "        unigrams.update(o)\n",
    "        for i in range(len(o) - 1):\n",
    "            bigrams.add(o[i] + '_' + o[i+1])\n",
    "        for i in range(len(o) - 2):\n",
    "            trigrams.add(o[i] + '_' + o[i+1] + '_' + o[i+2])\n",
    "    dist1 = (len(unigrams) / total_words)\n",
    "    dist2 = (len(bigrams) / total_words)\n",
    "    dist3 = (len(trigrams) / total_words)\n",
    "    \n",
    "    return dist1, dist2, dist3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005d2b4-8504-4333-bf99-092985c3be7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b519fb40-7a3f-44a8-9686-12724411f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_2(y_hat_tot_plus, y_tot):\n",
    "    \n",
    "    bleu_value = bleu_score(y_hat_tot_plus, y_tot)\n",
    "    try:\n",
    "        rouge_value = rouge_score(y_hat_tot_plus, y_tot)\n",
    "    except:\n",
    "        rouge_value = {\"rouge1\": 0.00}\n",
    "        \n",
    "                \n",
    "    return bleu_value, rouge_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25d1a6-9bbd-4c6d-bc3b-b6f07e17d953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4ea39-c2e8-4233-9065-26037341a937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf2da1-0563-4d61-895a-1bd207dcfcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8c935-ddb5-4721-8e94-df8568c374b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f592b-6fbe-4d6e-9e80-871363b7468f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8384bdd-b575-4a4f-8a39-c4ebab46e584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
